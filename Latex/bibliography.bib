
@article{devlin_bert_2018,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	file = {Full Text:C\:\\Users\\Megaport\\Zotero\\storage\\RSQZCABM\\Devlin et al. - 2018 - Bert Pre-training of deep bidirectional transform.pdf:application/pdf;Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\H9QQHYWH\\1810.html:text/html},
}

@article{kolthoff_data-driven_2023,
	title = {Data-driven prototyping via natural-language-based {GUI} retrieval},
	volume = {30},
	issn = {1573-7535},
	url = {doi.org/10.1007/s10515-023-00377-x},
	doi = {10.1007/s10515-023-00377-x},
	abstract = {Rapid GUI prototyping has evolved into a widely applied technique in early stages of software development to facilitate the clarification and refinement of requirements. Especially high-fidelity GUI prototyping has shown to enable productive discussions with customers and mitigate potential misunderstandings, however, the benefits of applying high-fidelity GUI prototypes are accompanied by the disadvantage of being expensive and time-consuming in development and requiring experience to create. In this work, we show RaWi, a data-driven GUI prototyping approach that effectively retrieves GUIs for reuse from a large-scale semi-automatically created GUI repository for mobile apps on the basis of Natural Language (NL) searches to facilitate GUI prototyping and improve its productivity by leveraging the vast GUI prototyping knowledge embodied in the repository. Retrieved GUIs can directly be reused and adapted in the graphical editor of RaWi. Moreover, we present a comprehensive evaluation methodology to enable (i) the systematic evaluation of NL-based GUI ranking methods through a novel high-quality gold standard and conduct an in-depth evaluation of traditional IR and state-of-the-art BERT-based models for GUI ranking, and (ii) the assessment of GUI prototyping productivity accompanied by an extensive user study in a practical GUI prototyping environment.},
	language = {en},
	number = {1},
	date = {2024-02-01},
	journal = {Automated Software Engineering},
	author = {Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo},
	month = mar,
	year = {2023},
	keywords = {Data-driven GUI prototyping, Deriving editable GUI screens, GUI prototypes from natural language requirements, Information retrieval for GUIs, Rapid prototyping of graphical user interfaces (GUIs)},
	pages = {13},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\4R3ASFRH\\Kolthoff et al. - 2023 - Data-driven prototyping via natural-language-based.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	date = {2024-02-08},
	publisher = {arXiv},
	author = {{OpenAI} and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	month = dec,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\8QL37XJS\\OpenAI et al. - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\LA7UX7WZ\\2303.html:text/html},
}

@inproceedings{alemerien_guievaluator_2014,
	title = {{GUIEvaluator}: {A} {Metric}-tool for {Evaluating} the {Complexity} of {Graphical} {User} {Interfaces}.},
	shorttitle = {{GUIEvaluator}},
	url = {www.academia.edu/download/77634934/seke14paper_115.pdf},
	date = {2024-02-09},
	booktitle = {{SEKE}},
	author = {Alemerien, Khalid and Magel, Kenneth},
	year = {2014},
	keywords = {GUI Evaluation},
	pages = {13--18},
	file = {Available Version (via Google Scholar):C\:\\Users\\Megaport\\Zotero\\storage\\CS4WG5QD\\Alemerien und Magel - 2014 - GUIEvaluator A Metric-tool for Evaluating the Com.pdf:application/pdf},
}

@book{johnson_designing_2020,
	title = {Designing with the {Mind} in {Mind}: {Simple} {Guide} to {Understanding} {User} {Interface} {Design} {Guidelines}},
	isbn = {978-0-12-818203-1},
	shorttitle = {Designing with the {Mind} in {Mind}},
	abstract = {User interface (UI) design rules and guidelines, developed by early HCI gurus and recognized throughout the field, were based on cognitive psychology (study of mental processes such as problem solving, memory, and language), and early practitioners were well informed of its tenets. But today practitioners with backgrounds in cognitive psychology are a minority, as user interface designers and developers enter the field from a wide array of disciplines. HCI practitioners today have enough experience in UI design that they have been exposed to UI design rules, but it is essential that they understand the psychological basis behind the rules in order to effectively apply them. In Designing with the Mind in Mind, best-selling author Jeff Johnson provides designers with just enough background in perceptual and cognitive psychology that UI design guidelines make intuitive sense rather than being just a list of rules to follow. Provides an essential source for user interface design rules and how, when, and why to apply them Arms designers with the science behind each design rule, allowing them to make informed decisions in projects, and to explain those decisions to others  Equips readers with the knowledge to make educated tradeoffs between competing rules, project deadlines, and budget pressures Completely updated and revised, including additional coverage in such areas as persuasion, cognitive economics and decision making, emotions, trust, habit formation, and speech UIs},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Johnson, Jeff},
	month = aug,
	year = {2020},
	note = {Google-Books-ID: \_dLVDwAAQBAJ},
	keywords = {Computers / Human-Computer Interaction (HCI), Computers / Internet / General, Computers / User Interfaces, Computers / Web / General, GUI Design},
}

@inproceedings{lee_guicomp_2020,
	address = {Honolulu HI USA},
	title = {{GUIComp}: {A} {GUI} {Design} {Assistant} with {Real}-{Time}, {Multi}-{Faceted} {Feedback}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{GUIComp}},
	url = {dl.acm.org/doi/10.1145/3313831.3376327},
	doi = {10.1145/3313831.3376327},
	language = {en},
	date = {2024-03-19},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lee, Chunggi and Kim, Sanghoon and Han, Dongyun and Yang, Hongjun and Park, Young-Woo and Kwon, Bum Chul and Ko, Sungahn},
	month = apr,
	year = {2020},
	pages = {1--13},
	file = {Available Version (via Google Scholar):C\:\\Users\\Megaport\\Zotero\\storage\\VT6LGERS\\Lee et al. - 2020 - GUIComp A GUI Design Assistant with Real-Time, Mu.pdf:application/pdf},
}

@inproceedings{soui_plain_2017,
	title = {Plain: {Plugin} for predicting the usability of mobile user interface},
	volume = {2},
	shorttitle = {Plain},
	url = {www.scitepress.org/PublishedPapers/2017/61712/},
	date = {2024-03-16},
	booktitle = {International {Conference} on {Computer} {Graphics} {Theory} and {Applications}},
	publisher = {SciTePress},
	author = {Soui, Makram and Chouchane, Mabrouka and Gasmi, Ines and Mkaouer, Mohamed Wiem},
	year = {2017},
	pages = {127--136},
	file = {Available Version (via Google Scholar):C\:\\Users\\Megaport\\Zotero\\storage\\FH6F9PKB\\Soui et al. - 2017 - Plain Plugin for predicting the usability of mobi.pdf:application/pdf},
}

@article{ngo_formalising_2000,
	title = {Formalising guidelines for the design of screen layouts},
	volume = {21},
	issn = {0141-9382},
	url = {www.sciencedirect.com/science/article/pii/S0141938200000263},
	doi = {10.1016/S0141-9382(00)00026-3},
	abstract = {An important aspect of screen design is aesthetic evaluation of screen layouts. While it is conceivable to define a set of variables that characterise the key attributes of many alphanumeric display formats, such a task seems difficult for graphic displays because of their much greater complexity. This article proposes a theoretical approach to capture the essence of artists’ insights with fourteen aesthetic measures for graphic displays. Our empirical study has suggested that these measures are important to prospective viewers and may help gain attention and build confidence in using computer systems.},
	number = {1},
	date = {2024-03-16},
	journal = {Displays},
	author = {Ngo, D. C. L. and Teo, L. S. and Byrne, J. G.},
	month = mar,
	year = {2000},
	keywords = {Aesthetic characteristics, Aesthetic measures, Interface aesthetics, Multi-screen interfaces, Screen design},
	pages = {3--15},
	file = {Ngo et al. - 2000 - Formalising guidelines for the design of screen la.pdf:C\:\\Users\\Megaport\\Zotero\\storage\\SETN8BYQ\\Ngo et al. - 2000 - Formalising guidelines for the design of screen la.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\WN8XS2WE\\S0141938200000263.html:text/html},
}

@article{riegler_measuring_2018,
	title = {Measuring visual user interface complexity of mobile applications with metrics},
	volume = {30},
	url = {academic.oup.com/iwc/article-abstract/30/3/207/4967896},
	number = {3},
	date = {2024-03-25},
	journal = {Interacting with Computers},
	author = {Riegler, Andreas and Holzmann, Clemens},
	year = {2018},
	note = {Publisher: Oxford University Press},
	pages = {207--223},
	file = {Available Version (via Google Scholar):C\:\\Users\\Megaport\\Zotero\\storage\\DDSP92KN\\IWCOMP_iwy008_final.pdf:application/pdf},
}

@inproceedings{duan_generating_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Generating {Automatic} {Feedback} on {UI} {Mockups} with {Large} {Language} {Models}},
	isbn = {9798400703300},
	url = {dl.acm.org/doi/10.1145/3613904.3642782},
	doi = {10.1145/3613904.3642782},
	abstract = {Feedback on user interface (UI) mockups is crucial in design. However, human feedback is not always readily available. We explore the potential of using large language models for automatic feedback. Specifically, we focus on applying GPT-4 to automate heuristic evaluation, which currently entails a human expert assessing a UI’s compliance with a set of design guidelines. We implemented a Figma plugin that takes in a UI design and a set of written heuristics, and renders automatically-generated feedback as constructive suggestions. We assessed performance on 51 UIs using three sets of guidelines, compared GPT-4-generated design suggestions with those from human experts, and conducted a study with 12 expert designers to understand fit with existing practice. We found that GPT-4-based feedback is useful for catching subtle errors, improving text, and considering UI semantics, but feedback also decreased in utility over iterations. Participants described several uses for this plugin despite its imperfect suggestions.},
	date = {2024-05-14},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Duan, Peitong and Warner, Jeremy and Li, Yang and Hartmann, Bjoern},
	month = may,
	year = {2024},
	keywords = {Computational UI Design Tools, Large Language Models},
	pages = {1--20},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\EU2BB7SL\\3613904.3642782.pdf:application/pdf},
}

@inproceedings{lu_ai_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {{AI} {Is} {Not} {Enough}: {A} {Hybrid} {Technical} {Approach} to {AI} {Adoption} in {UI} {Linting} {With} {Heuristics}},
	isbn = {9798400703317},
	shorttitle = {{AI} {Is} {Not} {Enough}},
	url = {dl.acm.org/doi/10.1145/3613905.3637135},
	doi = {10.1145/3613905.3637135},
	abstract = {Design systems have become an industry standard for creating consistent, usable, and effective digital interfaces. However, detecting and correcting violations of design system guidelines, known as UI linting, is a major challenge. Manual UI linting is time-consuming and tedious, making it a prime candidate for automation. This paper presents a case study of adopting AI for UI linting. Through collaborative prototyping with UX designers, we analyzed the limitations of existing AI models and identified designers’ core needs and priorities in UI linting. With such knowledge, we designed a hybrid technical pipeline that combines the deterministic nature of heuristics with the flexibility of large language models. Our case study demonstrates that AI alone is not sufficient for practical adoption and highlights the importance of a deep understanding of AI capabilities and user-centered design approaches.},
	date = {2024-05-14},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Yuwen and Knearem, Tiffany and Dutta, Shona and Blass, Jamie and Kliman-Silver, Clara and Bentley, Frank},
	month = may,
	year = {2024},
	keywords = {artificial intelligence, design systems, large language models, UI linting, user interface (UI) design},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\ULXD8YRA\\3613905.3637135.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	date = {2024-08-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\6X24J68J\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{noauthor_beautiful_nodate,
	title = {Beautiful {Soup}: {We} called him {Tortoise} because he taught us.},
	url = {www.crummy.com/software/BeautifulSoup/},
	date = {2024-08-09},
	file = {Beautiful Soup\: We called him Tortoise because he taught us.:C\:\\Users\\Megaport\\Zotero\\storage\\2LSE5RZ9\\BeautifulSoup.html:text/html},
}

@inproceedings{deka_rico_2017,
	address = {New York, NY, USA},
	series = {{UIST} '17},
	title = {Rico: {A} {Mobile} {App} {Dataset} for {Building} {Data}-{Driven} {Design} {Applications}},
	isbn = {978-1-4503-4981-9},
	shorttitle = {Rico},
	url = {dl.acm.org/doi/10.1145/3126594.3126651},
	doi = {10.1145/3126594.3126651},
	abstract = {Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports query- by-example search over UIs.},
	date = {2024-08-09},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
	month = oct,
	year = {2017},
	pages = {845--854},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\28MR4JIR\\Deka et al. - 2017 - Rico A Mobile App Dataset for Building Data-Drive.pdf:application/pdf},
}

@inproceedings{leiva_enrico_2021,
	address = {New York, NY, USA},
	series = {{MobileHCI} '20},
	title = {Enrico: {A} {Dataset} for {Topic} {Modeling} of {Mobile} {UI} {Designs}},
	isbn = {978-1-4503-8052-2},
	shorttitle = {Enrico},
	url = {dl.acm.org/doi/10.1145/3406324.3410710},
	doi = {10.1145/3406324.3410710},
	abstract = {Topic modeling of user interfaces (UIs), also known as layout design categorization, contributes to a better understanding of the UI functionality. Starting from Rico, a large dataset of mobile UIs, we revised a random sample of 10k UIs and concluded to Enrico (shorthand of Enhanced Rico), a human-supervised high-quality dataset comprising 1460 UIs and 20 design topics. As a validation example, we train a deep learning model for three different UI representations (screenshots, wireframes, and embeddings). The screenshot representation provides the highest discriminative power (95\% AUC) and a competitive accuracy of 75\% (a random classifier achieves 5\% accuracy in the same task). We discuss several applications that can be developed with this new public resource, including e.g. semantic UI captioning and tagging, explainable UI designs, smart tutorials, and improved design search capabilities.},
	date = {2024-08-09},
	booktitle = {22nd {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services}},
	publisher = {Association for Computing Machinery},
	author = {Leiva, Luis A. and Hota, Asutosh and Oulasvirta, Antti},
	month = feb,
	year = {2021},
	pages = {1--4},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\HL6VQJG8\\Leiva et al. - 2021 - Enrico A Dataset for Topic Modeling of Mobile UI .pdf:application/pdf},
}

@misc{gemini_team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	date = {2024-08-09},
	publisher = {arXiv},
	author = {{Gemini Team}},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\5SMYD6FP\\Gemini Team et al. - 2024 - Gemini A Family of Highly Capable Multimodal Mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\M23LDN2E\\2312.html:text/html},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	date = {2024-08-09},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\XLPSJJ3Z\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\3VXVNSUL\\2302.html:text/html},
}

@inproceedings{just_defects4j_2014,
	address = {New York, NY, USA},
	series = {{ISSTA} 2014},
	title = {{Defects4J}: a database of existing faults to enable controlled testing studies for {Java} programs},
	isbn = {978-1-4503-2645-2},
	shorttitle = {{Defects4J}},
	url = {dl.acm.org/doi/10.1145/2610384.2628055},
	doi = {10.1145/2610384.2628055},
	abstract = {Empirical studies in software testing research may not be comparable, reproducible, or characteristic of practice. One reason is that real bugs are too infrequently used in software testing research. Extracting and reproducing real bugs is challenging and as a result hand-seeded faults or mutants are commonly used as a substitute. This paper presents Defects4J, a database and extensible framework providing real bugs to enable reproducible studies in software testing research. The initial version of Defects4J contains 357 real bugs from 5 real-world open source pro- grams. Each real bug is accompanied by a comprehensive test suite that can expose (demonstrate) that bug. Defects4J is extensible and builds on top of each program’s version con- trol system. Once a program is configured in Defects4J, new bugs can be added to the database with little or no effort. Defects4J features a framework to easily access faulty and fixed program versions and corresponding test suites. This framework also provides a high-level interface to common tasks in software testing research, making it easy to con- duct and reproduce empirical studies. Defects4J is publicly available at http://defects4j.org.},
	date = {2024-08-09},
	booktitle = {Proceedings of the 2014 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Just, René and Jalali, Darioush and Ernst, Michael D.},
	month = jul,
	year = {2014},
	pages = {437--440},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\7VZCWJMV\\Just et al. - 2014 - Defects4J a database of existing faults to enable.pdf:application/pdf},
}

@misc{experience_10_nodate,
	title = {10 {Usability} {Heuristics} for {User} {Interface} {Design}},
	howpublished = {www.nngroup.com/articles/ten-usability-heuristics/},
	abstract = {Jakob Nielsen's 10 general principles for interaction design. They are called "heuristics" because they are broad rules of thumb and not specific usability guidelines.},
	language = {en},
	note = {Accessed: 2024-08-09},
	author = {{Nielsen Norman Group}},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\KJRM6K4R\\ten-usability-heuristics.html:text/html},
}

@misc{oven_corp_ovenappio_2024,
	title = {{KakaOven}},
	howpublished = {ovenapp.io},
	abstract = {Oven(오븐)은 HTML5 기반의 무료 웹/앱 프로토타이핑 툴입니다. (카카오 제공)},
	note = {Accessed: 2024-08-09},
	journal = {Oven},
	author = {Oven Corp},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\IJ2NJS4I\\ovenapp.io.html:text/html},
}

@misc{bouzenia_repairagent_2024,
	title = {{RepairAgent}: {An} {Autonomous}, {LLM}-{Based} {Agent} for {Program} {Repair}},
	shorttitle = {{RepairAgent}},
	url = {http://arxiv.org/abs/2403.17134},
	doi = {10.48550/arXiv.2403.17134},
	abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
	date = {2024-08-09},
	publisher = {arXiv},
	author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
	month = mar,
	year = {2024},
	note = {arXiv preprint arXiv:2403.17134 (2024)},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\Y69Y6KLX\\Bouzenia et al. - 2024 - RepairAgent An Autonomous, LLM-Based Agent for Pr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\VT79IHMG\\2403.html:text/html},
}

@misc{noauthor_free_nodate,
	title = {Free {Design} {Tools} to {Enhance} {Your} {Creative} {Process}},
	url = {www.figma.com/community/plugin/1135653849910773588/json-exporter},
	abstract = {Explore 1000+ design tools to enhance your creative process. From wireframe generators to color palette creators, find the perfect tools to bring your ideas to life.},
	language = {en},
	date = {2024-08-18},
	journal = {Figma},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\PPM2JYYZ\\json-exporter.html:text/html},
}

@inproceedings{feng_mud_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {{MUD}: {Towards} a {Large}-{Scale} and {Noise}-{Filtered} {UI} {Dataset} for {Modern} {Style} {UI} {Modeling}},
	isbn = {9798400703300},
	shorttitle = {{MUD}},
	url = {dl.acm.org/doi/10.1145/3613904.3642350},
	doi = {10.1145/3613904.3642350},
	abstract = {The importance of computational modeling of mobile user interfaces (UIs) is undeniable. However, these require a high-quality UI dataset. Existing datasets are often outdated, collected years ago, and are frequently noisy with mismatches in their visual representation. This presents challenges in modeling UI understanding in the wild. This paper introduces a novel approach to automatically mine UI data from Android apps, leveraging Large Language Models (LLMs) to mimic human-like exploration. To ensure dataset quality, we employ the best practices in UI noise filtering and incorporate human annotation as a final validation step. Our results demonstrate the effectiveness of LLMs-enhanced app exploration in mining more meaningful UIs, resulting in a large dataset MUD of 18k human-annotated UIs from 3.3k apps. We highlight the usefulness of MUD in two common UI modeling tasks: element detection and UI retrieval, showcasing its potential to establish a foundation for future research into high-quality, modern UIs.},
	date = {2024-08-18},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Sidong and Ma, Suyu and Wang, Han and Kong, David and Chen, Chunyang},
	month = may,
	year = {2024},
	pages = {1--14},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\YVVJLTEP\\Feng et al. - 2024 - MUD Towards a Large-Scale and Noise-Filtered UI D.pdf:application/pdf},
}

@inproceedings{zen_towards_2014,
	title = {Towards an evaluation of graphical user interfaces aesthetics based on metrics},
	url = {ieeexplore.ieee.org/abstract/document/6861050},
	doi = {10.1109/RCIS.2014.6861050},
	abstract = {The graphical user interface (GUI) of an interactive system is nowadays the most frequently used interaction modality. While the contents are of high importance, the Look and Feel is an equally essential factor determining the GUI quality that is impacted by several determinants such as but not limited to aesthetics, pleasurability, fun, etc. Therefore, GUIs aesthetics is a potential element to focus on in order to facilitate communication between device and user. On that basis, one question that comes up is: “Is it possible to evaluate the quality of a GUI by estimating its aesthetics through a series of measurable geometric metrics?”. This paper suggests possible directions to address the previous question by, first, introducing a simplifying model of GUIs aesthetics that captures aesthetics aspects and regions-related metrics. In a second phase, a methodology for the evaluation of GUIs aesthetics is defined based on the underlying model. The paper finally puts forwards a model-based implementation of the aforementioned methodology in the form of a web service tool for metrics-based evaluation of GUIs and discuss the results of a survey on users aesthetics perceptions.},
	date = {2024-08-21},
	booktitle = {2014 {IEEE} {Eighth} {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	author = {Zen, Mathieu and Vanderdonckt, Jean},
	month = may,
	year = {2014},
	note = {ISSN: 2151-1357},
	keywords = {Computational modeling, Visualization, Graphical user interfaces, Usability, Measurement, Layout, Color, metrics-based evaluation, user interface aesthetics},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Megaport\\Zotero\\storage\\92FZXF8Z\\6861050.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\M9DPATZP\\Zen und Vanderdonckt - 2014 - Towards an evaluation of graphical user interfaces.pdf:application/pdf},
}

@article{odonovan_learning_2014,
	title = {Learning {Layouts} for {Single}-{PageGraphic} {Designs}},
	volume = {20},
	issn = {1941-0506},
	url = {ieeexplore.ieee.org/abstract/document/6777138},
	doi = {10.1109/TVCG.2014.48},
	abstract = {This paper presents an approach for automatically creating graphic design layouts using a new energy-based model derived from design principles. The model includes several new algorithms for analyzing graphic designs, including the prediction of perceived importance, alignment detection, and hierarchical segmentation. Given the model, we use optimization to synthesize new layouts for a variety of single-page graphic designs. Model parameters are learned with Nonlinear Inverse Optimization (NIO) from a small number of example layouts. To demonstrate our approach, we show results for applications including generating design layouts in various styles, retargeting designs to new sizes, and improving existing designs. We also compare our automatic results with designs created using crowdsourcing and show that our approach performs slightly better than novice designers.},
	number = {8},
	date = {2024-08-20},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {O’Donovan, Peter and Agarwala, Aseem and Hertzmann, Aaron},
	month = aug,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Computational modeling, Layout, Algorithm design and analysis, crowdsourcing, Face, Graphic design, layout, learning, modeling, nonlinear inverse optimization, Optimization, Predictive models},
	pages = {1200--1213},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Megaport\\Zotero\\storage\\CYP6CWEC\\6777138.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\IUIW5VDM\\O’Donovan et al. - 2014 - Learning Layouts for Single-PageGraphic Designs.pdf:application/pdf},
}

@inproceedings{bylinskii_learning_2017,
	address = {New York, NY, USA},
	series = {{UIST} '17},
	title = {Learning {Visual} {Importance} for {Graphic} {Designs} and {Data} {Visualizations}},
	isbn = {978-1-4503-4981-9},
	url = {dl.acm.org/doi/10.1145/3126594.3126653},
	doi = {10.1145/3126594.3126653},
	abstract = {Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.},
	date = {2024-08-20},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Bylinskii, Zoya and Kim, Nam Wook and O'Donovan, Peter and Alsheikh, Sami and Madan, Spandan and Pfister, Hanspeter and Durand, Fredo and Russell, Bryan and Hertzmann, Aaron},
	month = oct,
	year = {2017},
	pages = {57--69},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\SXYZVYQ9\\Bylinskii et al. - 2017 - Learning Visual Importance for Graphic Designs and.pdf:application/pdf},
}

@misc{khot_decomposed_2023,
	title = {Decomposed {Prompting}: {A} {Modular} {Approach} for {Solving} {Complex} {Tasks}},
	shorttitle = {Decomposed {Prompting}},
	url = {http://arxiv.org/abs/2210.02406},
	doi = {10.48550/arXiv.2210.02406},
	abstract = {Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at github.com/allenai/DecomP.},
	date = {2024-08-20},
	publisher = {arXiv},
	author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
	month = apr,
	year = {2023},
	note = {arXiv:2210.02406 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR'23 Camera Ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\E5DKEPRX\\Khot et al. - 2023 - Decomposed Prompting A Modular Approach for Solvi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\YU52TRN9\\2210.html:text/html},
}

@inproceedings{reynolds_prompt_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Prompt {Programming} for {Large} {Language} {Models}: {Beyond} the {Few}-{Shot} {Paradigm}},
	isbn = {978-1-4503-8095-9},
	shorttitle = {Prompt {Programming} for {Large} {Language} {Models}},
	url = {dl.acm.org/doi/10.1145/3411763.3451760},
	doi = {10.1145/3411763.3451760},
	abstract = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
	date = {2024-08-20},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Reynolds, Laria and McDonell, Kyle},
	month = may,
	year = {2021},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\AVE6P8XC\\Reynolds und McDonell - 2021 - Prompt Programming for Large Language Models Beyo.pdf:application/pdf},
}

@misc{noauthor_prompt_nodate,
	title = {Prompt {Programming} for {Large} {Language} {Models}: {Beyond} the {Few}-{Shot} {Paradigm} {\textbar} {Extended} {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {dl.acm.org/doi/abs/10.1145/3411763.3451760},
	date = {2024-08-20},
	file = {Prompt Programming for Large Language Models\: Beyond the Few-Shot Paradigm | Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems:C\:\\Users\\Megaport\\Zotero\\storage\\2YDCA79D\\3411763.html:text/html},
}

@article{liu_pre-train_2023,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {dl.acm.org/doi/10.1145/3560815},
	doi = {10.1145/3560815},
	abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,\\&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
	number = {9},
	date = {2024-08-20},
	journal = {ACM Comput. Surv.},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jan,
	year = {2023},
	pages = {195:1--195:35},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\DK2WRRB9\\Liu et al. - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf:application/pdf},
}

@misc{riccardi_two_2023,
	title = {The {Two} {Word} {Test}: {A} {Semantic} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {The {Two} {Word} {Test}},
	url = {http://arxiv.org/abs/2306.04610},
	doi = {10.48550/arXiv.2306.04610},
	abstract = {Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, with both versions. Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the limitations and weaknesses of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing 'true understanding' or AGI to LLMs. TWT is available at: github.com/NickRiccardi/two-word-test},
	date = {2024-08-20},
	publisher = {arXiv},
	author = {Riccardi, Nicholas and Desai, Rutvik H.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.04610 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 12 pages, 5 figures, 3 tables, submitted to NeurIPS 2023 Datasets and Benchmarks Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\WVPFP2GU\\Riccardi und Desai - 2023 - The Two Word Test A Semantic Benchmark for Large .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\WIK6EKAU\\2306.html:text/html},
}

@misc{tao_eveval_2023,
	title = {{EvEval}: {A} {Comprehensive} {Evaluation} of {Event} {Semantics} for {Large} {Language} {Models}},
	shorttitle = {{EvEval}},
	url = {http://arxiv.org/abs/2305.15268},
	doi = {10.48550/arXiv.2305.15268},
	abstract = {Events serve as fundamental units of occurrence within various contexts. The processing of event semantics in textual information forms the basis of numerous natural language processing (NLP) applications. Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain. Furthermore, the lack of a comprehensive evaluation framework for event semantic processing poses a significant challenge in evaluating these capabilities. In this paper, we propose an overarching framework for event semantic processing, encompassing understanding, reasoning, and prediction, along with their fine-grained aspects. To comprehensively evaluate the event semantic processing abilities of models, we introduce a novel benchmark called EVEVAL. We collect 8 datasets that cover all aspects of event semantic processing. Extensive experiments are conducted on EVEVAL, leading to several noteworthy findings based on the obtained results.},
	date = {2024-08-20},
	publisher = {arXiv},
	author = {Tao, Zhengwei and Jin, Zhi and Bai, Xiaoying and Zhao, Haiyan and Feng, Yanlin and Li, Jia and Hu, Wenpeng},
	month = may,
	year = {2023},
	note = {arXiv:2305.15268 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\JMZSEHDJ\\Tao et al. - 2023 - EvEval A Comprehensive Evaluation of Event Semant.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\9CNDHJSY\\2305.html:text/html},
}

@article{chang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {15},
	issn = {2157-6904},
	url = {dl.acm.org/doi/10.1145/3641289},
	doi = {10.1145/3641289},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
	number = {3},
	date = {2024-08-20},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = mar,
	year = {2024},
	pages = {39:1--39:45},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\SJMC5CL8\\Chang et al. - 2024 - A Survey on Evaluation of Large Language Models.pdf:application/pdf},
}

@misc{minaee_large_2024,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	date = {2024-08-20},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2401.14423},
	file = {arXiv Fulltext PDF:C\:\\Users\\Megaport\\Zotero\\storage\\HVEAP3JL\\Minaee et al. - 2024 - Large Language Models A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\D5S59UBN\\2402.html:text/html},
}

@inproceedings{lian_imperfect_2024,
	address = {New York, NY, USA},
	series = {{ICSE}-{Companion} '24},
	title = {Imperfect {Code} {Generation}: {Uncovering} {Weaknesses} in {Automatic} {Code} {Generation} by {Large} {Language} {Models}},
	isbn = {9798400705021},
	shorttitle = {Imperfect {Code} {Generation}},
	url = {dl.acm.org/doi/10.1145/3639478.3643081},
	doi = {10.1145/3639478.3643081},
	abstract = {The task of code generation has received significant attention in recent years, especially when the pre-trained large language models (LLMs) for code have consistently achieved state-of-the-art performance. However, there is currently a lack of a comprehensive weakness taxonomy in the field, uncovering weaknesses in automatic code generation by LLMs. This may lead the community to invest excessive efforts into well-known hotspots while neglecting many crucial yet unrecognized issues that deserve more attention. To bridge this gap, we conduct a systematic study on analyzing the weaknesses based on three state-of-the-art LLMs across three widely-used code generation datasets. Our study identifies eight types of weaknesses and assesses their prevalence across each LLM and dataset, aiming to inform and shape the trajectory of future research in the domain.},
	date = {2024-08-20},
	booktitle = {Proceedings of the 2024 {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Lian, Xiaoli and Wang, Shuaisong and Ma, Jieping and Tan, Xin and Liu, Fang and Shi, Lin and Gao, Cuiyun and Zhang, Li},
	month = may,
	year = {2024},
	pages = {422--423},
	file = {Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\8HFU45VU\\Lian et al. - 2024 - Imperfect Code Generation Uncovering Weaknesses i.pdf:application/pdf},
}

@inproceedings{yang_dont_2021,
	title = {Don’t {Do} {That}! {Hunting} {Down} {Visual} {Design} {Smells} in {Complex} {UIs} {Against} {Design} {Guidelines}},
	url = {ieeexplore.ieee.org/abstract/document/9402139},
	doi = {10.1109/ICSE43902.2021.00075},
	abstract = {Just like code smells in source code, UI design has visual design smells. We study 93 don't-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don't-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design. The detection accuracy of UIS-Hunter is high (precision=0.81, recall=0.90) on the 60,756 UIs of 9,286 apps. We build a guideline gallery with real-world UI design smells that UIS-Hunter detects for developers to learn the best Material Design practices. Our user studies show that UIS-Hunter is more effective than manual detection of UI design smells, and the UI design smells that are detected by UIS-Hunter have severely negative impacts on app users.},
	date = {2024-08-19},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Yang, Bo and Xing, Zhenchang and Xia, Xin and Chen, Chunyang and Ye, Deheng and Li, Shanping},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Training, Visualization, Software engineering, Tools, GUI testing, Guidelines, Design methodology, Color, Material design, UI design smell, Violation detection},
	pages = {761--772},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Megaport\\Zotero\\storage\\RZSUV4KY\\9402139.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Megaport\\Zotero\\storage\\M5GK2RRV\\Yang et al. - 2021 - Don’t Do That! Hunting Down Visual Design Smells i.pdf:application/pdf},
}

@book{constantine_software_1999,
	title = {Software for {Use}: {A} {Practical} {Guide} to the {Models} and {Methods} of {Usage}-{Centered} {Design}},
	isbn = {978-0-7686-8498-8},
	shorttitle = {Software for {Use}},
	abstract = {In the quest for quality, software developers have long focused on improving the internal architecture of their products. Larry L. Constantine--who originally created structured design to effect such improvement--now joins with well-known consultant Lucy A. D. Lockwood to turn the focus of software development to the external architecture. In this book, they present the models and methods of a revolutionary approach to software that will help programmers deliver more usable software--software that will enable users to accomplish their tasks with greater ease and efficiency.  Recognizing usability as the key to successful software, Constantine and Lockwood provide concrete tools and techniques that programmers can employ to meet that end. Much more than just another set of rules for good user-interface design, this book guides readers through a systematic software development process. This process, called usage-centered design, weaves together two major threads in software development methods: use cases (also used with UML) and essential modeling. With numerous examples and case studies of both conventional and specialized software applications, the authors illustrate what has been shown in practice to work and what has proved to be of greatest practical value.  Highlights   Presents a streamlined process for developing highly usable software  Describes practical methods and models successfully implemented in industry  Complements modern development practices, including the Unified Process and other object-oriented software engineering approaches},
	language = {en},
	publisher = {Pearson Education},
	author = {Constantine, Larry L. and Lockwood, Lucy A. D.},
	month = apr,
	year = {1999},
	note = {Google-Books-ID: Y0ic4kK9E7cC},
	keywords = {Computers / Software Development \\& Engineering / General},
}

@book{stone_user_2005,
	title = {User {Interface} {Design} and {Evaluation}},
	isbn = {978-0-08-052032-2},
	abstract = {User Interface Design and Evaluation provides an overview of the user-centered design field. It illustrates the benefits of a user-centered approach to the design of software, computer systems, and websites. The book provides clear and practical discussions of requirements gathering, developing interaction design from user requirements, and user interface evaluation. The book's coverage includes established HCI topics—for example, visibility, affordance, feedback, metaphors, mental models, and the like—combined with practical guidelines for contemporary designs and current trends, which makes for a winning combination. It provides a clear presentation of ideas, illustrations of concepts, using real-world applications. This book will help readers develop all the skills necessary for iterative user-centered design, and provides a firm foundation for user interface design and evaluation on which to build. It is ideal for seasoned professionals in user interface design and usability engineering (looking for new tools with which to expand their knowledge); new people who enter the HCI field with no prior educational experience; and software developers, web application developers, and information appliance designers who need to know more about interaction design and evaluation. Co-published by the Open University, UK. Covers the design of graphical user interfaces, web sites, and interfaces for embedded systems. Full color production, with activities, projects, hundreds of illustrations, and industrial applications.},
	language = {en},
	publisher = {Elsevier},
	author = {Stone, Debbie and Jarrett, Caroline and Woodroffe, Mark and Minocha, Shailey},
	month = apr,
	year = {2005},
	note = {Google-Books-ID: VvSoyqPBPbMC},
	keywords = {Computers / Human-Computer Interaction (HCI), Computers / Software Development \\& Engineering / General, Computers / User Interfaces, Computers / Operating Systems / General},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	howpublished = {s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf},
	date = {2024-08-24},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	note = {Publisher: San Francisco, CA, USA},
	file = {Available Version (via Google Scholar):C\:\\Users\\Megaport\\Zotero\\storage\\UFH9TMC6\\Radford et al. - 2018 - Improving language understanding by generative pre.pdf:application/pdf},
}

@misc{noauthor_openai_nodate,
	author = {OpenAI},
	title = {{OpenAI} {Platform}},
	howpublished = {platform.openai.com},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\TVP88NE9\\overview.html:text/html},
}

@misc{noauthor_material_nodate,
	author = {Material Design},
	title = {Material {Design} Homepage},
	howpublished = {m3.material.io},
	abstract = {Build beautiful, usable products faster. Material Design is an adaptable system—backed by open-source code—that helps teams build high quality digital experiences.},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Material Design},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\49PW73JW\\m3.material.io.html:text/html},
}

@misc{noauthor_one_nodate,
	author = {Samsung},
	title = {One {UI}},
	howpublished = {www.samsung.com/de/one-ui/},
	abstract = {One UI 6 erlaubt es dir, deinen Tag optimal zu gestalten, indem du Dinge einfach über verschiedene Galaxy-Geräte hinweg erledigst.},
	language = {de-DE},
	note = {Accessed: 2024-08-24},
	journal = {Samsung de},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\USZKEH8Y\\one-ui.html:text/html},
}

@misc{inc_design_nodate,
	author = {Apple},
	title = {Apple Design},
	howpublished = {developer.apple.com/design/},
	abstract = {Find documentation and resources for designing great apps for Apple platforms.},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Apple Developer},
	author = {Inc, Apple},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\RVU5UGS6\\design.html:text/html},
}

@misc{noauthor_figma_nodate,
	author = {Figma},
	title = {Figma Homepage},
	howpublished = {www.figma.com/de-de/},
	note = {Accessed: 2024-08-24},
	file = {Figma\: Das kollaborative Designtool für Benutzeroberflächen:C\:\\Users\\Megaport\\Zotero\\storage\\ARHQ5YX4\\de-de.html:text/html},
}

@misc{noauthor_figma_nodate-1,
	author = {Figma},
	title = {Figma {Community} Homepage},
	shorttitle = {Figma {Community}},
	howpublished = {www.figma.com/community},
	abstract = {Explore, install and use thousands of templates, plugins, and widgets published to the Figma Community by designers and developers.},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Figma},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\QYYCN76K\\community.html:text/html},
}

@misc{noauthor_material_nodate-1,
	author = {Material Design},
	title = {Material 3 {Design} {Kit}},
	howpublished = {www.figma.com/community/file/1035203688168086460/material-3-design-kit},
	abstract = {Introducing Material Design 3
Meet Material Design 3, Material Design’s most personal design system yet. The Material 3 Design Kit provides a comprehensive introduction to the design system, with styles and components to help you get started.

Visualize dynamic color in your UI
The Material 3 Des...},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Figma},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\F4Q4YZ8D\\material-3-design-kit.html:text/html},
}

@misc{noauthor_material_nodate-2,
	author = {Material Design},
	title = {Material {Theme} {Builder}},
	howpublished = {www.figma.com/community/plugin/1034969338659738588/material-theme-builder},
	abstract = {Type theming is here!
Now you can customize your fonts along with color.

* Playground file added with instructions and color schematic! Try it out by clicking Open In… + Playground file*

The Material Theme builder is built to assist in exploring the possibilities of dynamic color, harmonizing b...},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Figma},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\VGWZDR6S\\material-theme-builder.html:text/html},
}

@misc{noauthor_whatsapp_nodate,
	author = {WhatsApp},
	title = {{WhatsApp} Homepage},
	howpublished = {www.whatsapp.com},
	abstract = {Bleibe über den WhatsApp Messenger mit Freund*innen und Familie in Kontakt. WhatsApp ist kostenlos auf Mobiltelefonen rund um die Welt verfügbar und bietet die Möglichkeit, einfach, sicher und zuverlässig Nachrichten auszutauschen und zu telefonieren.},
	language = {de},
	note = {Accessed: 2024-08-24},
	journal = {WhatsApp.com},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\ACFA7FUW\\www.whatsapp.com.html:text/html},
}

@misc{noauthor_instagram_nodate,
	author = {Instagram},
	title = {Instagram Homepage},
	howpublished = {instagram.com/},
	abstract = {Erstelle ein Konto oder melde dich bei Instagram an – Teile deine Ideen mit Menschen, die so ticken wie du.},
	language = {de},
	note = {Accessed: 2024-08-24},
	journal = {Instagram},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\8HDUMWZJ\\www.instagram.com.html:text/html},
}

@misc{noauthor_json_nodate,
	author = {Yunser},
	title = {{JSON} {Exporter} Plugin},
	howpublished = {\linebreak www.figma.com/community/plugin/1135653849910773588/json-exporter},
	abstract = {Export layer to JSON Text.

导出图层为 JSON 文本。},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Figma},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\Q3AAEG4G\\json-exporter.html:text/html},
}

@misc{noauthor_beautiful_nodate-1,
	author = {Richardson, Leonard},
	title = {{Beautiful} {Soup} 4.12.0 documentation},
	howpublished = {www.crummy.com/software/BeautifulSoup/bs4/doc/},
	note = {Accessed: 2024-08-24},
	file = {Beautiful Soup Documentation — Beautiful Soup 4.12.0 documentation:C\:\\Users\\Megaport\\Zotero\\storage\\A2IN93V5\\doc.html:text/html},
}

@misc{noauthor_letterboxd_nodate,
	author = {Google},
	title = {Letterboxd Play Store Page},
	howpublished = {play.google.com/store/apps/details?id=com.letterboxd.letterboxd\&hl=de},
	abstract = {Das beliebte soziale Netzwerk für Filmliebhaber, direkt in Ihrer Tasche.},
	language = {de},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\C2UNLAXQ\\details.html:text/html},
}

@inproceedings{pyterrier2020ictir,
	author = {Craig Macdonald and Nicola Tonellotto},
	title = {Declarative Experimentation inInformation Retrieval using PyTerrier},
	booktitle = {Proceedings of ICTIR 2020},
	year = {2020}
}

@misc{noauthor_ast_nodate,
	title = {ast — {Abstract} {Syntax} {Trees}},
	howpublished = {docs.python.org/3/library/ast.html},
	abstract = {Source code: Lib/ast.py The ast module helps Python applications to process trees of the Python abstract syntax grammar. The abstract syntax itself might change with each Python release; this modul...},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Python documentation},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\D8ERQJJ7\\ast.html:text/html},
}

@misc{noauthor_tricount_nodate,
	author = {Tricount},
	title = {Tricount Homepage},
	howpublished = {tricount.com/},
	abstract = {The solution for organizing group expenses - Perfect when you organize group activities like trips, holidays, meals or when you share flat expenses with roommates},
	language = {en},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\LQCPA4U8\\tricount.com.html:text/html},
}

@misc{runtastic_adidas_nodate,
	author = {Adidas},
	title = {adidas {Running} Homepage},
	howpublished = {www.runtastic.com},
	abstract = {Werde aktiv und bleib motiviert mit der adidas Running App. Trainiere für einen 5- oder 10-km-Lauf, einen Halbmarathon oder Marathon, mit einem Plan der genau für dich gemacht ist. Mit ständig neuem Content hilft dir adidas Running dabei, an dein Ziel zu kommen.},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\K2ZZJII6\\www.runtastic.com.html:text/html},
}

@misc{noauthor_food_nodate,
	author = {Grubhub},
	title = {Grubhub Homepage},
	howpublished = {www.grubhub.com},
	abstract = {The best restaurants near you deliver with Grubhub! Order delivery or takeout from national chains and local favorites! Help support your neighborhood restaurants.},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Grubhub},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\3FNV8VGI\\www.grubhub.com.html:text/html},
}

@misc{noauthor_bottom_nodate,
	author = {Material Design},
	title = {Bottom app bar – {Material} {Design} 3},
	howpublished = {m3.material.io/components/bottom-app-bar/guidelines},
	abstract = {A bottom app bar displays navigation and key actions at the bottom of mobile screens.},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Material Design},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\UJITKKT6\\guidelines.html:text/html},
}

@misc{noauthor_carousel_nodate,
	author = {Material Design},
	title = {Carousel – {Material} {Design} 3},
	howpublished = {m3.material.io/components/carousel/guidelines},
	abstract = {Carousels show a collection of items that can be scrolled on and off the screen},
	language = {en},
	note = {Accessed: 2024-08-24},
	journal = {Material Design}
}

@misc{noauthor_reddit_nodate,
	author = {Google},
	title = {{Reddit} {Play} {Store} {Page}},
	howpublished = {\linebreak play.google.com/store/apps/details?id=com.reddit.frontpage\&hl=en},
	abstract = {Join your forum community to discuss trending topics in social media threads},
	language = {en},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\WVXYFLLK\\details.html:text/html},
}

@misc{noauthor_amazon_nodate,
	author = {Google},
	title = {Amazon {Prime} {Video} Play Store Page},
	howpublished = {play.google.com/store/apps/details?id=com.amazon.avod.thirdpartyclient\&hl=de},
	abstract = {Sehen Sie Filme und Serien, darunter preisgekrönte Amazon Originals.},
	language = {de},
	note = {Accessed: 2024-08-24},
	file = {Snapshot:C\:\\Users\\Megaport\\Zotero\\storage\\2JZT3YNX\\details.html:text/html},
}

@misc{noauthor_amazon_nodate-1,
	author = {Amazon},
	title = {Amazon {Mechanical} {Turk}},
	howpublished = {www.mturk.com/},
	note = {Accessed: 2024-08-24},
	file = {Amazon Mechanical Turk:C\:\\Users\\Megaport\\Zotero\\storage\\B3QU8283\\www.mturk.com.html:text/html},
}

@misc{openai-gpt4o,
	author = {OpenAI},
	title = {{OpenAI API Documentation on GPT-4o}},
	howpublished = {platform.openai.com/docs/models/gpt-4o},
	note = {Accessed: 2024-08-29}
}

@misc{xia_keep_2023,
	title = {Keep the {Conversation} {Going}: {Fixing} 162 out of 337 bugs for \$0.42 each using {ChatGPT}},
	shorttitle = {Keep the {Conversation} {Going}},
	url = {http://arxiv.org/abs/2304.00385},
	doi = {10.48550/arXiv.2304.00385},
	abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches. To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for {\textbackslash}\$0.42 each!},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Xia, Chunqiu Steven and Zhang, Lingming},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00385 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\thoma\\Zotero\\storage\\LXULJ6ED\\Xia und Zhang - 2023 - Keep the Conversation Going Fixing 162 out of 337.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\thoma\\Zotero\\storage\\ZFWIN2RC\\2304.html:text/html},
}

@inproceedings{lin_quixbugs_2017,
	address = {New York, NY, USA},
	series = {{SPLASH} {Companion} 2017},
	title = {{QuixBugs}: a multi-lingual program repair benchmark set based on the quixey challenge},
	isbn = {978-1-4503-5514-8},
	shorttitle = {{QuixBugs}},
	url = {dl.acm.org/doi/10.1145/3135932.3135941},
	doi = {10.1145/3135932.3135941},
	abstract = {Recent years have seen an explosion of work in automated program repair. While previous work has focused exclusively on tools for single languages, recent work in multi-language transformation has opened the door for multi-language program repair tools. Evaluating the performance of such a tool requires having a benchmark set of similar buggy programs in different languages. We present QuixBugs, consisting of 40 programs translated to both Python and Java, each with a bug on a single line. The QuixBugs benchmark suite is based on problems from the Quixey Challenge, where programmers were given a short buggy program and 1 minute to fix the bug.},
	urldate = {2024-08-26},
	booktitle = {Proceedings {Companion} of the 2017 {ACM} {SIGPLAN} {International} {Conference} on {Systems}, {Programming}, {Languages}, and {Applications}: {Software} for {Humanity}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Derrick and Koppel, James and Chen, Angela and Solar-Lezama, Armando},
	month = oct,
	year = {2017},
	pages = {55--56},
	file = {Full Text PDF:C\:\\Users\\thoma\\Zotero\\storage\\CDTDACLR\\Lin et al. - 2017 - QuixBugs a multi-lingual program repair benchmark.pdf:application/pdf},
}


@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	urldate = {2024-08-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\thoma\\Zotero\\storage\\MY8TXBQB\\Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf:application/pdf},
}

@article{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	language = {en},
	urldate = {2024-08-28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
	month = dec,
	year = {2022},
	pages = {27730--27744},
	file = {Full Text PDF:C\:\\Users\\thoma\\Zotero\\storage\\QVHY8BVI\\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf},
}

@misc{singhal_long_2024,
	title = {A {Long} {Way} to {Go}: {Investigating} {Length} {Correlations} in {RLHF}},
	shorttitle = {A {Long} {Way} to {Go}},
	url = {http://arxiv.org/abs/2310.03716},
	doi = {10.48550/arXiv.2310.03716},
	abstract = {Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for "helpfulness" in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg},
	month = jul,
	year = {2024},
	note = {arXiv:2310.03716 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 21 pages, 13 figures, Accepted to COLM 2024},
	file = {arXiv Fulltext PDF:C\:\\Users\\thoma\\Zotero\\storage\\CXSAJ3FU\\Singhal et al. - 2024 - A Long Way to Go Investigating Length Correlation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\thoma\\Zotero\\storage\\ZFE3CDJT\\2310.html:text/html},
}
